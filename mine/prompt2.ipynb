{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGHCHAIN_TRACINB_V2 = \"true\"\n",
    "LANGCHAIN_API_KEY = \"ls__ef7c69da49cf48719e4d7c4b33a3debd\"\n",
    "import os\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model='qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In the legal domain, rhetorical roles refer to the various positions that individuals may take when participating in a legal argument or debate. These roles involve persuading others of the validity and importance of specific points of view or arguments. Some common rhetorical roles in the legal domain include:\\n\\n1. Advocate: An advocate is an individual who represents a party in a legal proceeding, arguing on their behalf to present the strongest possible case. They use rhetoric to persuade the judge, jury, or other decision-maker of the merits of their client's position.\\n\\n2. Prosecutor: A prosecutor is responsible for presenting the case against a defendant in a criminal trial. Their role involves using rhetoric to convince the jury or judge that the defendant is guilty beyond a reasonable doubt and that they should be convicted of the charges.\\n\\n3. Defense Attorney: A defense attorney represents the interests of the accused in a criminal case, working to prove their innocence or mitigate the severity of the punishment. They use rhetoric to argue for reasonable doubt or to present alternative explanations for the evidence presented by the prosecution.\\n\\n4. Judge: The judge presides over the legal proceedings and ensures that they are conducted fairly and in accordance with the law. They may also play a role in shaping the rhetorical landscape of the case by ruling on objections, guiding the attorneys through procedural matters, and sometimes offering clarifications or interpretations of the law.\\n\\n5. Witness: A witness is an individual who provides testimony during a legal proceeding to help establish facts or provide information relevant to the case. They may be called upon to testify in support of either the prosecution or defense, depending on their knowledge and connection to the case.\\n\\n6. Expert Witness: An expert witness is an individual with specialized knowledge or expertise in a particular field who provides opinions or testimony relevant to the case. Their role involves using rhetoric to persuade the court of the validity of their expert opinion.\\n\\n7. Mediator: A mediator assists parties in resolving disputes outside of the formal legal system, often in civil cases. They use rhetorical skills to facilitate communication and help the parties reach a mutually agreeable resolution.\\n\\nEach of these roles involves different perspectives and objectives within the legal domain, but all rely on rhetorical skills to effectively present their arguments and persuade others of the validity and importance of their points of view.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What are rhetorical roles in legal domain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': 'preamble',\n",
       " 'FAC': 'facts',\n",
       " 'RLC': 'ruling by lower court',\n",
       " 'ISSUE': 'issues',\n",
       " 'ARG_PETITIONER': 'argument by petitioner',\n",
       " 'ARG_RESPONDENT': 'argument by respondent',\n",
       " 'ANALYSIS': 'analysis',\n",
       " 'STA': 'statute ',\n",
       " 'PRE_RELIED': 'precedent relied',\n",
       " 'PRE_NOT_RELIED': 'precedent not relied',\n",
       " 'Ratio': 'ratio of the decision',\n",
       " 'RPC': 'ruling by present court',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'statute ': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "\n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RLC']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query = dataset[0]['annotations'][0]['result'][22]['value']['text']\n",
    "sample_label = dataset[0]['annotations'][0]['result'][22]['value']['labels']\n",
    "sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RLC'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['annotations'][0]['result'][21]['value']['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruling by lower court\n"
     ]
    }
   ],
   "source": [
    "prev_sent =  dataset[0]['annotations'][0]['result'][21]['value']['text']\n",
    "prev_label = abbrv_label[dataset[0]['annotations'][0]['result'][21]['value']['labels'][0].strip()]\n",
    "curr_sent = sample_query\n",
    "print(prev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    output = ['Preamble']\n",
    "    for i in tqdm(range(1, len(data))):\n",
    "        \n",
    "        curr_sent = dataset[0]['annotations'][0]['result'][i]['value']['text']\n",
    "        prev_sent = dataset[0]['annotations'][0]['result'][i-1]['value']['text']\n",
    "        if i==1:\n",
    "            prev_label = 'Preamble'\n",
    "        prompt = f'''You are a specialized system focused on semantic annotation of court opinion.\n",
    "        RHETORICAL ROLE:\n",
    "        Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "        a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "        roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "        conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "        Your task is to label each sentence in the document with one of the following predefined rhetorical roles: ['Preamble', 'Facts',\n",
    "        'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute',\n",
    "        'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "        The definition of each rhetorical role is given below:\n",
    "        Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "        Facts: \tThis refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.). Depositions and proceedings of current court. Summary of lower court proceedings.\n",
    "        Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "        Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES.\n",
    "        Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "        Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "        Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS\n",
    "        Statute : Text in which the court discusses Established laws, which can come from a mixture of sources – Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute\n",
    "        Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent\n",
    "        Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.\n",
    "        Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum.\n",
    "        Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale\n",
    "        NONE: If a sentence does not belong to any of the above categories.\n",
    "        \n",
    "        The prediction of the current sentence highly depends on the previous sentence. Most of the time it's same but not always.\n",
    "        \n",
    "        \n",
    "        You will be given these information to classify the rhetorical role of the current sentence: \n",
    "        1. The previous sentence.\n",
    "        2. The predicted rhetorical role of the previous sentence that that you classified.\n",
    "\n",
    "        Here is the information to help you predict the rhetorical role:\n",
    "        The previous sentence: {prev_sent}\n",
    "        The label of the previous sentence: {prev_label}\n",
    "\n",
    "        Only output the rhetorical role as string.\n",
    "        Now do prediction for this sentence:\n",
    "        {curr_sent}\n",
    "        '''\n",
    "\n",
    "        response = llm.invoke(prompt).strip()\n",
    "        prev_label = response\n",
    "        output.append(response)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:26<00:00,  3.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = run_preds(dataset[0]['annotations'][0]['result'])\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "true_labels = joblib.load('./vec_prompt/true_labels')\n",
    "len(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Analysis',\n",
       "  'Argument by Petitioner',\n",
       "  'Argument by Petitioner',\n",
       "  'Analysis',\n",
       "  'Ruling by Present Court',\n",
       "  'Argument by Petitioner',\n",
       "  'Argument by Respondent',\n",
       "  'NONE',\n",
       "  'NONE',\n",
       "  \"'NONE'\"],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_labels = []\n",
    "for i, label in enumerate(preds):\n",
    "    try:\n",
    "        llm_labels.append(label_abbrv[label.lower().replace(\"'\", \"\")])\n",
    "    except:\n",
    "        print(i, label.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ANALYSIS',\n",
       "  'ARG_PETITIONER',\n",
       "  'ARG_PETITIONER',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'ARG_PETITIONER',\n",
       "  'ARG_RESPONDENT',\n",
       "  'NONE',\n",
       "  'NONE',\n",
       "  'NONE'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "acc = accuracy_score(true_labels, llm_labels)\n",
    "f1 = f1_score(true_labels, llm_labels, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27472527472527475, 0.24326777087646653)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['FAC', 'FAC', 'FAC', 'FAC', 'FAC', 'FAC', 'FAC', 'FAC', 'FAC', 'FAC'],\n",
       " ['FAC',\n",
       "  'ARG_RESPONDENT',\n",
       "  'ARG_RESPONDENT',\n",
       "  'ARG_RESPONDENT',\n",
       "  'ARG_RESPONDENT',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[10:20], true_labels[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.24      0.56      0.33         9\n",
      "ARG_PETITIONER       0.14      0.18      0.16        11\n",
      "ARG_RESPONDENT       0.00      0.00      0.00        17\n",
      "           FAC       0.32      0.92      0.48        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       0.20      0.50      0.29         2\n",
      "      PREAMBLE       1.00      0.75      0.86         4\n",
      "    PRE_RELIED       0.50      0.04      0.07        26\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       1.00      0.25      0.40         4\n",
      "           RPC       1.00      0.20      0.33         5\n",
      "         Ratio       0.00      0.00      0.00         0\n",
      "\n",
      "      accuracy                           0.27        91\n",
      "     macro avg       0.37      0.28      0.24        91\n",
      "  weighted avg       0.37      0.27      0.22        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, llm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take top k sentences then pair them like this <sent1> <label1>....\n",
    "# do sentence level classificattion with custom labels.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
