{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "api_base = 'https://api.endpoints.anyscale.com/v1'\n",
    "token = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "url = f\"{api_base}/chat/completions\"\n",
    "\n",
    "def get_completion(query, model='meta-llama/Meta-Llama-3-70B-Instruct'):\n",
    "    body = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"system\", \"content\": \"You are a specialized system focused on sentence classification of court opinion.\"},\n",
    "                {\"role\": \"user\", \"content\": query}],\n",
    "    \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    with s.post(url, headers={\"Authorization\": f\"Bearer {token}\"}, json=body) as resp:\n",
    "        output = resp.json()['choices'][0]['message']['content']\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 91)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "top_preds = joblib.load('./vecs/top_preds')\n",
    "true_labels = joblib.load('./vecs/true_labels')\n",
    "doc_labels = joblib.load('./vecs/doc_label')\n",
    "\n",
    "len(top_preds), len(true_labels), len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in top_preds:\n",
    "    if len(p)!=5:\n",
    "        print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    preds = []\n",
    "    for i in tqdm(range(3, len(data))):\n",
    "        curr_sent = data[i]['value']['text']\n",
    "\n",
    "        prompt = f'''RHETORICAL ROLE:\n",
    "Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "Your task is to label each sentence in the document with one of the following predefined rhetorical roles: \n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', \\\n",
    "'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "The definition of each rhetorical role is given below:\n",
    "Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "Facts: This refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.) Depositions and proceedings of current court. Summary of lower court proceedings\n",
    "Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES. E.g. “he point emerge for determination is as follow:- (i) Whether on 06.08.2017 the accused persons in furtherance of their common intention intentionally caused the death of the deceased by assaulting him by means of axe ?”\n",
    "Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for petitioner argued that …”\n",
    "Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for the respondent argued that …”\n",
    "Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS. E.g. “Post Mortem Report establishes that .. “ E.g. “In view of the abovementioned findings, it is evident that the ingredients of Section 307 have been made out ….”\n",
    "Statute : Text in which the court discusses Established laws, which can come from a mixture of sources - Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute. E.g. “Court had referred to Section 4 of the Code, which reads as under: \"4. Trial of offences under the Indian Penal Code and other laws.-- (1) All offences under the Indian Penal Code (45 of 1860) shall be investigated, inquired into, tried, and otherwise dealt with according to the provisions hereinafter contained”\n",
    "Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum. E.g. “The finding that the sister concern is eligible for more deduction under Section 80HHC of the Act is based on mere surmise and conjectures also does not arise for consideration.”\n",
    "Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale. E.g. “In the result, we do not find any merit in this appeal. The same fails and is hereby dismissed.”\n",
    "NONE: If a sentence does not belong to any of the above categories. E.g. “We have considered the submissions made by learned counsel for the parties and have perused the record.”\n",
    "        \n",
    "The label of current sentence also depends on the previous sentences. So you will be provided with the last 3 sentences and the labels you previously predicted.\n",
    "\n",
    "'''\n",
    "        for j in range(i-3, i):\n",
    "            sent = data[j]['value']['text']\n",
    "            if i==3:\n",
    "                label = data[j]['value']['labels'][0]\n",
    "                preds.append(label)\n",
    "            else:\n",
    "                label = preds[j]\n",
    "            prompt+=f\"SENTENCE: {sent.strip()}\" + f\"\\nLABEL: {label}\\n\"\n",
    "        \n",
    "        prompt+=\"\\nThe label of the current sentence also depends on the label of the most similar sentences so you are given top 5 most similar sentences and their corresponding labels:\\n\"\n",
    "        for p in top_preds[i]:\n",
    "            prompt+= f\"SENTENCE: {p[0].strip()}\" + f\"\\nLABEL: {p[1]}\\n\"\n",
    "\n",
    "        pred_prompt = f'''You are given one sentence from the document your task is to predict the label of that sentence.\n",
    "You must use the last three sentences and their labels to predict the label of the current sentence.\n",
    "You must use the most similar sentences and their labels to predict the label of the current sentence.\n",
    "You should first think about how you will solve this problem and use that explanation to get your predicted output.\n",
    "Your output should be in json format in backticks like this:\n",
    "```\n",
    "{{\"explanation\": \"\", \"label\": \"\"}}\n",
    "```\n",
    "{curr_sent.strip()}'''\n",
    "\n",
    "        prompt += pred_prompt\n",
    "    \n",
    "        response = get_completion(prompt).strip()\n",
    "        \n",
    "        # print(response)\n",
    "        # break\n",
    "        preds.append(response)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [13:56<00:00,  9.51s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = run_preds(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREAMBLE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'PREAMBLE')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs[0]), true_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation': \"The current sentence is describing the marriage of Lakshmi, which is similar to the sentences labeled as 'Facts' that provide background information about the case. The sentence structure and content are also similar to the top 5 most similar sentences provided, which are all labeled as 'Facts'. Additionally, the previous sentences and their labels also suggest that the current sentence is providing factual information about the case. Therefore, I predict that the label for the current sentence is 'Facts'.\",\n",
       " 'label': 'Facts'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser = JsonOutputParser()\n",
    "output_parser.invoke(outputs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREAMBLE\n",
      "PREAMBLE\n",
      "PREAMBLE\n",
      "PREAMBLE\n",
      "PREAMBLE\n",
      "PREAMBLE\n",
      "```\n",
      "{\"explanation\": \"The current sentence is discussing a precedent case cited by the learned Counsel for the appellant, which is a characteristic of sentences labeled as 'Precedent Relied'. The sentence structure and content are similar to the previous sentences labeled as 'Precedent Relied' that discuss prior case documents, discussions, and decisions relied upon by the court. The most similar sentence provided, such as 'This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, …..\" is labeled as 'Precedent Relied'. The current sentence is providing a discussion of a precedent case relied upon by the court, which is a common characteristic of sentences labeled as 'Precedent Relied'. Therefore, I predict that the label for the current sentence is 'Precedent Relied'.\", \"label\": \"Precedent Relied\"}\n",
      "```\n",
      "I predicted the label of the current sentence as 'Precedent Relied' because it is discussing a precedent case cited by the learned Counsel for the appellant, similar to the previous sentences labeled as 'Precedent Relied'. The sentence structure and content are also similar to those previous sentences, which supports the prediction of the label as 'Precedent Relied'.\n",
      "PRE_RELIED\n",
      "```\n",
      "{\"explanation\": \"The current sentence is discussing the analysis of a concept ('instigation') and its nuances, which is similar to the sentences labeled as 'Analysis' that provide the court's views on evidence, facts, and laws. The sentence structure and content are similar to the previous sentences labeled as 'Analysis', such as 'To satisfy the requirement of instigation though it is not necessary that actual words must be used to that effect or what constitutes instigation must necessarily and specifically be suggestive of the consequence.' and 'Yet a reasonable certainty to incite the consequence must be capable of being spelt out.' The most similar sentence provided, 'The word is also frequently used without the foregoing implications in statutes and inter parties to exclude or save transactions, acts and rights from the consequences of a stated proposition and so as to mean not affection', 'saving' or 'excepting\"', is also labeled as 'Analysis', which further supports this prediction. Therefore, I predict that the label for the current sentence is 'Analysis'.\", \"label\": \"Analysis\"}\n",
      "```\n",
      "PRE_RELIED\n",
      "```\n",
      "{\"explanation\": \"The current sentence is discussing a precedent case (Chitresh Kumar Chopra vs. State) and its relevance to the present case, which is similar to the sentences labeled as 'Precedent Relied' that discuss prior case documents, discussions, and decisions relied upon by the court for final decisions. The sentence structure and content are similar to the previous sentences labeled as 'Precedent Relied', such as 'In State of West Bengal vs. Orilal Jaiswal, (1994)1 SCC 73, the apex court has expressed that the court should be extremely careful in assessing the facts and circumstances of each case and the evidence adduced in the trial court for the purpose of finding whether cruelty meted out to the victim had in fact induced her to end life by committing suicide.' The most similar sentence provided, '16. In Union of India & Ors. v. Ex. Flt. Lt. G.S. Bajwa2, this Court examined the issue pertaining to the authority competent to convene the Court Martial and held as follows: \"44.', is also labeled as 'Precedent Relied', which further supports this prediction. Therefore, I predict that the label for the current sentence is 'Precedent Relied'.\", \"label\": \"Precedent Relied\"}\n",
      "```\n",
      "PRE_RELIED\n"
     ]
    }
   ],
   "source": [
    "llm_labels = ['PREAMBLE', 'PREAMBLE', 'PREAMBLE']\n",
    "for i, label in enumerate(outputs):\n",
    "    try:\n",
    "        label_dict = output_parser.invoke(label)\n",
    "        llm_labels.append(label_dict['label'])\n",
    "    except:\n",
    "        print(label)\n",
    "        print(true_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Analysis',\n",
       "  'Analysis',\n",
       "  'Analysis',\n",
       "  'Analysis',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'NONE'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'Facts',\n",
       "  'Facts',\n",
       "  'Facts',\n",
       "  'Facts',\n",
       "  'Facts'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[:10], true_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': \"'preamble'\",\n",
       " 'FAC': 'fac',\n",
       " 'RLC': 'rlc',\n",
       " 'ISSUE': 'issue',\n",
       " 'ARG_PETITIONER': 'arg_petitioner',\n",
       " 'ARG_RESPONDENT': 'arg_respondent',\n",
       " 'ANALYSIS': 'pre_analysis',\n",
       " 'STA': 'statute',\n",
       " 'PRE_RELIED': 'pre_relied',\n",
       " 'PRE_NOT_RELIED': 'pre_not_relied',\n",
       " 'Ratio': 'ratio_of_decision',\n",
       " 'RPC': 'rpc',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    \"'preamble'\": 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'fac': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'rlc': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'issue': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'argument_by_petitioner': 'ARG_PETITIONER',\n",
    "    'arg_petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'arg_respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'pre_analysis': 'ANALYSIS',\n",
    "    'statute': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'pre_relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'pre_not_relied': 'PRE_NOT_RELIED',\n",
    "    'ratio': 'Ratio',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ratio of decision': 'Ratio',\n",
    "    'ratio_of_decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'ruling_by_present_court': 'RPC',\n",
    "    'rpc': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "  \n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for i, label in enumerate(llm_labels):\n",
    "    try:\n",
    "        preds.append(label_abbrv[label.lower().strip()])\n",
    "        trues.append(true_labels[i])\n",
    "    except:\n",
    "        # preds.append(\"none\")\n",
    "        # trues.append(true_labels[i].lower())\n",
    "        print(i, label.lower())\n",
    "        print(true_labels[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10], trues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "acc = accuracy_score(trues, preds)\n",
    "f1 = f1_score(trues, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3522727272727273, 0.345729836237787)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.21      0.78      0.33         9\n",
      "ARG_PETITIONER       1.00      0.18      0.31        11\n",
      "ARG_RESPONDENT       0.25      0.06      0.10        17\n",
      "           FAC       0.38      0.92      0.54        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       0.33      1.00      0.50         1\n",
      "      PREAMBLE       1.00      1.00      1.00         4\n",
      "    PRE_RELIED       0.67      0.08      0.14        26\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       1.00      0.25      0.40         4\n",
      "           RPC       0.40      0.67      0.50         3\n",
      "\n",
      "      accuracy                           0.35        88\n",
      "     macro avg       0.48      0.45      0.35        88\n",
      "  weighted avg       0.55      0.35      0.29        88\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
