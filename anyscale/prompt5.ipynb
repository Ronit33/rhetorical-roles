{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "api_base = 'https://api.endpoints.anyscale.com/v1'\n",
    "token = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "url = f\"{api_base}/chat/completions\"\n",
    "\n",
    "def get_completion(query, model='meta-llama/Meta-Llama-3-70B-Instruct'):\n",
    "    body = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"system\", \"content\": \"You are a specialized system focused on sentence classification of court opinion.\"},\n",
    "                {\"role\": \"user\", \"content\": query}],\n",
    "    \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    with s.post(url, headers={\"Authorization\": f\"Bearer {token}\"}, json=body) as resp:\n",
    "        output = resp.json()['choices'][0]['message']['content']\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 91)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "top_preds = joblib.load('./vecs/top_preds')\n",
    "true_labels = joblib.load('./vecs/true_labels')\n",
    "doc_labels = joblib.load('./vecs/doc_label')\n",
    "\n",
    "len(top_preds), len(true_labels), len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in top_preds:\n",
    "    if len(p)!=5:\n",
    "        print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    preds = []\n",
    "    for i in tqdm(range(3, len(data))):\n",
    "        curr_sent = data[i]['value']['text']\n",
    "\n",
    "        prompt = f'''RHETORICAL ROLE:\n",
    "Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "Your task is to label each sentence in the document with one of the following predefined rhetorical roles: \n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', \\\n",
    "'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "The definition of each rhetorical role is given below:\n",
    "Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "Facts: This refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.) Depositions and proceedings of current court. Summary of lower court proceedings\n",
    "Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES. E.g. “he point emerge for determination is as follow:- (i) Whether on 06.08.2017 the accused persons in furtherance of their common intention intentionally caused the death of the deceased by assaulting him by means of axe ?”\n",
    "Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for petitioner argued that …”\n",
    "Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for the respondent argued that …”\n",
    "Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS. E.g. “Post Mortem Report establishes that .. “ E.g. “In view of the abovementioned findings, it is evident that the ingredients of Section 307 have been made out ….”\n",
    "Statute : Text in which the court discusses Established laws, which can come from a mixture of sources - Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute. E.g. “Court had referred to Section 4 of the Code, which reads as under: \"4. Trial of offences under the Indian Penal Code and other laws.-- (1) All offences under the Indian Penal Code (45 of 1860) shall be investigated, inquired into, tried, and otherwise dealt with according to the provisions hereinafter contained”\n",
    "Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum. E.g. “The finding that the sister concern is eligible for more deduction under Section 80HHC of the Act is based on mere surmise and conjectures also does not arise for consideration.”\n",
    "Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale. E.g. “In the result, we do not find any merit in this appeal. The same fails and is hereby dismissed.”\n",
    "NONE: If a sentence does not belong to any of the above categories. E.g. “We have considered the submissions made by learned counsel for the parties and have perused the record.”\n",
    "        \n",
    "The label of current sentence also depends on the previous sentences. So you will be provided with the last 3 sentences and the labels you previously predicted.\n",
    "\n",
    "        '''\n",
    "        for j in range(i-3, i):\n",
    "            sent = data[j]['value']['text']\n",
    "            if i==3:\n",
    "                label = data[j]['value']['labels'][0]\n",
    "                preds.append(label)\n",
    "            else:\n",
    "                label = preds[j]\n",
    "            prompt+=f\"SENTENCE: {sent.strip()}\" + f\"\\nLABEL: {label}\\n\"\n",
    "        \n",
    "        prompt+=\"\\nThe label of the current sentence also depends on the label of the most similar sentences so you are given top 5 most similar sentences and their corresponding labels:\\n\"\n",
    "        for p in top_preds[i]:\n",
    "            prompt+= f\"SENTENCE: {p[0].strip()}\" + f\"\\nLABEL: {p[1]}\\n\"\n",
    "\n",
    "        pred_prompt = f'''You are given one sentence from the document your task is to predict the label of that sentence.\n",
    "You must use the last three sentences and their labels to predict the label of the current sentence.\n",
    "You must use the most similar sentences and their labels to predict the label of the current sentence.\n",
    "Your output should only be the label of the current sentence as a string and nothing else.\n",
    "Now do prediction for this current sentence:\n",
    "{curr_sent.strip()}'''\n",
    "\n",
    "        prompt += pred_prompt\n",
    "    \n",
    "        response = get_completion(prompt, model='mistralai/Mixtral-8x22B-Instruct-v0.1').strip()\n",
    "        preds.append(response)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [03:06<00:00,  2.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels = run_preds(dataset[0]['annotations'][0]['result'])\n",
    "len(llm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': \"'preamble'\",\n",
       " 'FAC': 'fac',\n",
       " 'RLC': 'rlc',\n",
       " 'ISSUE': 'issue',\n",
       " 'ARG_PETITIONER': 'arg_petitioner',\n",
       " 'ARG_RESPONDENT': 'arg_respondent',\n",
       " 'ANALYSIS': 'pre_analysis',\n",
       " 'STA': 'statute',\n",
       " 'PRE_RELIED': 'pre_relied',\n",
       " 'PRE_NOT_RELIED': 'pre_not_relied',\n",
       " 'Ratio': 'ratio_of_decision',\n",
       " 'RPC': 'rpc',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    \"'preamble'\": 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'fac': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'rlc': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'issue': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'argument_by_petitioner': 'ARG_PETITIONER',\n",
    "    'arg_petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'arg_respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'pre_analysis': 'ANALYSIS',\n",
    "    'statute': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'pre_relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'pre_not_relied': 'PRE_NOT_RELIED',\n",
    "    'ratio': 'Ratio',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ratio of decision': 'Ratio',\n",
    "    'ratio_of_decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'ruling_by_present_court': 'RPC',\n",
    "    'rpc': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "  \n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 analysis (based on the context provided, the sentence appears to be part of the court's analysis of the case)\n",
      "arg_respondent\n",
      "64 analysis\n",
      "\n",
      "explanation:\n",
      "the current sentence discusses a previous case and the interpretation of a legal term, which aligns with the definition of the 'analysis' label. the court is analyzing the concept of \"instigation,\" which is a legal term, and discussing its various meanings. additionally, the most similar sentence provided with the label 'analysis' supports the prediction that the current sentence also falls under the 'analysis' category.\n",
      "pre_relied\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for i, label in enumerate(llm_labels):\n",
    "    try:\n",
    "        preds.append(label_abbrv[label.lower()])\n",
    "        trues.append(true_labels[i])\n",
    "    except:\n",
    "        # preds.append(\"none\")\n",
    "        # trues.append(true_labels[i].lower())\n",
    "        print(i, label.lower())\n",
    "        print(true_labels[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 89)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'ARG_PETITIONER',\n",
       "  'ARG_RESPONDENT',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10], trues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "acc = accuracy_score(trues, preds)\n",
    "f1 = f1_score(trues, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42696629213483145, 0.3982045698531205)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.23      1.00      0.38         9\n",
      "ARG_PETITIONER       0.86      0.55      0.67        11\n",
      "ARG_RESPONDENT       0.14      0.06      0.09        16\n",
      "           FAC       0.56      0.83      0.67        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       1.00      0.50      0.67         2\n",
      "      PREAMBLE       1.00      1.00      1.00         4\n",
      "    PRE_RELIED       0.50      0.04      0.07        25\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       0.50      0.25      0.33         4\n",
      "           RPC       0.83      1.00      0.91         5\n",
      "         Ratio       0.00      0.00      0.00         0\n",
      "\n",
      "      accuracy                           0.43        89\n",
      "     macro avg       0.47      0.44      0.40        89\n",
      "  weighted avg       0.51      0.43      0.37        89\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
