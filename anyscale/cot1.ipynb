{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "api_base = 'https://api.endpoints.anyscale.com/v1'\n",
    "token = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "url = f\"{api_base}/chat/completions\"\n",
    "\n",
    "def get_completion(query, model='meta-llama/Meta-Llama-3-70B-Instruct'):\n",
    "    body = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"system\", \"content\": \"You are a specialized system focused on sentence classification of court opinion.\"},\n",
    "                {\"role\": \"user\", \"content\": query}],\n",
    "    \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    with s.post(url, headers={\"Authorization\": f\"Bearer {token}\"}, json=body) as resp:\n",
    "        output = resp.json()['choices'][0]['message']['content']\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 91)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "top_preds = joblib.load('./vecs/top_preds')\n",
    "true_labels = joblib.load('./vecs/true_labels')\n",
    "doc_labels = joblib.load('./vecs/doc_label')\n",
    "\n",
    "len(top_preds), len(true_labels), len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model='openchat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in top_preds:\n",
    "    if len(p)!=5:\n",
    "        print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    preds = []\n",
    "    for i in tqdm(range(3, len(data))):\n",
    "        curr_sent = data[i]['value']['text']\n",
    "\n",
    "        prompt = f'''RHETORICAL ROLE:\n",
    "Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "Your task is to label each sentence in the document with one of the following predefined rhetorical roles: \n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', \\\n",
    "'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "The definition of each rhetorical role is given below:\n",
    "Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "Facts: This refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.) Depositions and proceedings of current court. Summary of lower court proceedings\n",
    "Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES. E.g. “he point emerge for determination is as follow:- (i) Whether on 06.08.2017 the accused persons in furtherance of their common intention intentionally caused the death of the deceased by assaulting him by means of axe ?”\n",
    "Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for petitioner argued that …”\n",
    "Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category. E.g. “learned counsel for the respondent argued that …”\n",
    "Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS. E.g. “Post Mortem Report establishes that .. “ E.g. “In view of the abovementioned findings, it is evident that the ingredients of Section 307 have been made out ….”\n",
    "Statute : Text in which the court discusses Established laws, which can come from a mixture of sources - Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute. E.g. “Court had referred to Section 4 of the Code, which reads as under: \"4. Trial of offences under the Indian Penal Code and other laws.-- (1) All offences under the Indian Penal Code (45 of 1860) shall be investigated, inquired into, tried, and otherwise dealt with according to the provisions hereinafter contained”\n",
    "Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.. E.g. This Court in Jage Ram v. State of Haryana3 held that: \"12. For the purpose of conviction under Section 307 IPC, ….. “\n",
    "Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum. E.g. “The finding that the sister concern is eligible for more deduction under Section 80HHC of the Act is based on mere surmise and conjectures also does not arise for consideration.”\n",
    "Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale. E.g. “In the result, we do not find any merit in this appeal. The same fails and is hereby dismissed.”\n",
    "NONE: If a sentence does not belong to any of the above categories. E.g. “We have considered the submissions made by learned counsel for the parties and have perused the record.”\n",
    "        \n",
    "The label of current sentence also depends on the previous sentences. So you will be provided with the last 3 sentences and the labels you previously predicted.\n",
    "\n",
    "'''\n",
    "        for j in range(i-3, i):\n",
    "            sent = data[j]['value']['text']\n",
    "            if i==3:\n",
    "                label = data[j]['value']['labels'][0]\n",
    "                preds.append(label)\n",
    "            else:\n",
    "                label = preds[j]\n",
    "            prompt+=f\"SENTENCE: {sent.strip()}\" + f\"\\nLABEL: {label}\\n\"\n",
    "        \n",
    "        prompt+=\"\\nThe label of the current sentence also depends on the label of the most similar sentences so you are given top 5 most similar sentences and their corresponding labels:\\n\"\n",
    "        for p in top_preds[i]:\n",
    "            prompt+= f\"SENTENCE: {p[0].strip()}\" + f\"\\nLABEL: {p[1]}\\n\"\n",
    "\n",
    "        pred_prompt = f'''You are given one sentence from the document your task is to predict the label of that sentence.\n",
    "You must use the last three sentences and their labels to predict the label of the current sentence.\n",
    "You must use the most similar sentences and their labels to predict the label of the current sentence.\n",
    "Now before predicting the output first lets just think step by step process of how you will solve this problem for the following sentence:\n",
    "{curr_sent.strip()}'''\n",
    "\n",
    "        prompt += pred_prompt\n",
    "    \n",
    "        response = get_completion(prompt).strip()\n",
    "        \n",
    "        preds.append(response)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_preds(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88/88 [40:27<00:00, 27.59s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions = run_preds(dataset[0]['annotations'][0]['result'])\n",
    "len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the step-by-step process to predict the label for the given sentence:\n",
      "\n",
      "**Step 1: Analyze the sentence**\n",
      "The sentence is: \"This appeal coming on for hearing this day, the court delivered the following :- JUDGMENT\"\n",
      "\n",
      "**Step 2: Consider the last three sentences and their labels**\n",
      "The last three sentences and their labels are:\n",
      "1. \"This Criminal Appeal is filed under Section 374(2) of the Code of Criminal Procedure, 1973 by the advocate for the appellant praying to set aside the order of conviction and sentence in S.C.No.232/2008 on the file of the II Additional Sessions Judge, Gulbarga and acquit the appellant.\" - Label: PREAMBLE\n",
      "2. \"AND: The State of Karnataka                     .. RESPONDENT (Through Ratkal Police Station) Represented by Additional State Public Prosecutor, Circuit Bench, Gulbarga. (By Shri S.S.Aspalli, Government Pleader)\" - Label: PREAMBLE\n",
      "3. \"BEFORE THE HON'BLE MR.JUSTICE ANAND BYRAREDDY CRIMINAL APPEAL NO.3532 OF 2012 BETWEEN:                                            R\" - Label: PREAMBLE\n",
      "\n",
      "**Step 3: Identify the context and purpose of the sentence**\n",
      "The sentence appears to be introducing the judgment or order of the court, which is a typical component of a legal document, particularly in the introduction or preamble section.\n",
      "\n",
      "**Step 4: Consider the most similar sentences and their labels**\n",
      "The most similar sentences and their labels are:\n",
      "1. \"This Appeal coming on for Hearing this day, the Court delivered the following:- ORDER\" - Label: PREAMBLE\n",
      "2. \"appeal 30-11-2013 Date of Judgment\" - Label: PREAMBLE\n",
      "3. \"subject-matter of the appeal pending in this Court against the earlier judgment.\" - Label: PREAMBLE\n",
      "4. \"It is hence contended that the appeal be allowed and the judgment of the Tribunal be set aside.\" - Label: ANALYSIS\n",
      "5. \"On this report, the appeal again came up for hearing before the Tribunal.\" - Label: FAC (not relevant in this case)\n",
      "\n",
      "**Step 5: Predict the label**\n",
      "Based on the analysis, context, and similar sentences, I predict that the label for the sentence \"This appeal coming on for hearing this day, the court delivered the following :- JUDGMENT\" is **PREAMBLE**.\n"
     ]
    }
   ],
   "source": [
    "print(instructions[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAC\n"
     ]
    }
   ],
   "source": [
    "post_cot_prompt = f'''You are given some instructions.\n",
    "Your task is to analyse the instructions and pick the final label that is predicted from the instructions.\n",
    "Your final label should be from one of the following predefined rhetorical roles: \n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', \\\n",
    "'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "Your output should only be the label and nothing else.\n",
    "These are the instructions:\n",
    "{instructions[8]}'''\n",
    "print(llm.invoke(post_cot_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_cot(instructions):\n",
    "    final_preds = []\n",
    "    for instruction in tqdm(instructions, total=len(instructions)):\n",
    "        post_cot_prompt = f'''You are given some instructions.\n",
    "Your task is to analyse the instructions and pick the final from the instructions.\n",
    "Your final label should be from one of the following predefined rhetorical roles: \n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', \\\n",
    "'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "Only return the prediction and nothing else.\n",
    "These are the instructions:\n",
    "{instruction}'''\n",
    "        final_pred = llm.invoke(post_cot_prompt)\n",
    "        final_preds.append(final_pred)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [01:05<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "llm_labels = post_cot(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' ANALYSIS',\n",
       "  ' ANALYSIS',\n",
       "  ' ANALYSIS',\n",
       "  ' ANALYSIS',\n",
       "  ' Ruling by Present Court',\n",
       "  ' RULING_BY_PRESENT_COURT',\n",
       "  ' RULING_BY_PRESENT_COURT',\n",
       "  ' RULING_BY_PRESENT_COURT',\n",
       "  ' RULING_BY_PRESENT_COURT',\n",
       "  ' NONE'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': \"'preamble'\",\n",
       " 'FAC': 'fac',\n",
       " 'RLC': 'rlc',\n",
       " 'ISSUE': 'issue',\n",
       " 'ARG_PETITIONER': 'arg_petitioner',\n",
       " 'ARG_RESPONDENT': 'arg_respondent',\n",
       " 'ANALYSIS': 'pre_analysis',\n",
       " 'STA': 'statute',\n",
       " 'PRE_RELIED': 'pre_relied',\n",
       " 'PRE_NOT_RELIED': 'pre_not_relied',\n",
       " 'Ratio': 'ratio_of_decision',\n",
       " 'RPC': 'rpc',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    \"'preamble'\": 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'fac': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'rlc': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'issue': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'argument_by_petitioner': 'ARG_PETITIONER',\n",
    "    'arg_petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'arg_respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'pre_analysis': 'ANALYSIS',\n",
    "    'statute': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'pre_relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'pre_not_relied': 'PRE_NOT_RELIED',\n",
    "    'ratio': 'Ratio',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ratio of decision': 'Ratio',\n",
    "    'ratio_of_decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'ruling_by_present_court': 'RPC',\n",
    "    'rpc': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "  \n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34  analysis (analysis)\n",
      "arg_petitioner\n",
      "39  precedent_reli\n",
      "arg_respondent\n",
      "40  analysis (analysis)\n",
      "arg_respondent\n",
      "46  analysis (analysis)\n",
      "arg_respondent\n",
      "48  analysis (analysis)\n",
      "arg_respondent\n",
      "49  analysis (analysis)\n",
      "analysis\n",
      "51  analysis (analysis)\n",
      "analysis\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for i, label in enumerate(llm_labels):\n",
    "    try:\n",
    "        preds.append(label_abbrv[label.lower().strip()])\n",
    "        trues.append(true_labels[i])\n",
    "    except:\n",
    "        # preds.append(\"none\")\n",
    "        # trues.append(true_labels[i].lower())\n",
    "        print(i, label.lower())\n",
    "        print(true_labels[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10], trues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "acc = accuracy_score(trues, preds)\n",
    "f1 = f1_score(trues, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4642857142857143, 0.4611664197871094)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.22      1.00      0.36         7\n",
      "ARG_PETITIONER       1.00      0.50      0.67        10\n",
      "ARG_RESPONDENT       1.00      0.08      0.14        13\n",
      "           FAC       0.41      0.92      0.56        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       0.67      1.00      0.80         2\n",
      "      PREAMBLE       1.00      1.00      1.00         4\n",
      "    PRE_RELIED       1.00      0.12      0.21        26\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       0.50      0.25      0.33         4\n",
      "           RPC       1.00      1.00      1.00         5\n",
      "\n",
      "      accuracy                           0.46        84\n",
      "     macro avg       0.62      0.53      0.46        84\n",
      "  weighted avg       0.81      0.46      0.42        84\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
