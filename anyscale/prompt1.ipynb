{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "api_base = 'https://api.endpoints.anyscale.com/v1'\n",
    "token = os.getenv(\"ANYSCALE_API_KEY\")\n",
    "url = f\"{api_base}/chat/completions\"\n",
    "\n",
    "def get_completion(query, model='meta-llama/Meta-Llama-3-70B-Instruct'):\n",
    "    body = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": query}],\n",
    "    \"temperature\": 0.7\n",
    "    }\n",
    "\n",
    "    with s.post(url, headers={\"Authorization\": f\"Bearer {token}\"}, json=body) as resp:\n",
    "        output = resp.json()['choices'][0]['message']['content']\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(query='Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit? I'm all ears!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 91)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "top_preds = joblib.load('./vecs/top_preds')\n",
    "true_labels = joblib.load('./vecs/true_labels')\n",
    "doc_labels = joblib.load('./vecs/doc_label')\n",
    "\n",
    "len(top_preds), len(true_labels), len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['     IN THE HIGH COURT OF KARNATAKA\\n                  DHARWAD BENCH\\n\\nDATED THIS THE 21ST DAY OF SEPTEMBER, 2015',\n",
       "  'PREAMBLE'],\n",
       " ['IN THE HIGH COURT OF KARNATAKA AT BENGALURU\\n\\nDATED THIS THE 22ND DAY OF DECEMBER 2016\\n\\nBEFORE',\n",
       "  'PREAMBLE'],\n",
       " ['IN THE HIGH COURT OF KARNATAKA AT BENGALURU\\n         DATED THIS THE 13TH DAY OF JUNE 2019\\n                                   BEFORE',\n",
       "  'PREAMBLE'],\n",
       " ['IN THE HIGH COURT OF KARNATAKA AT BENGALURU\\n\\nDATED THIS THE 26TH DAY OF FEBRUARY 2016',\n",
       "  'PREAMBLE'],\n",
       " ['IN THE HIGH COURT OF KARNATAKA AT BENGALURU\\n    DATED THIS THE 29TH DAY OF JANUARY, 2015\\n                                  BEFORE',\n",
       "  'PREAMBLE']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in top_preds:\n",
    "    if len(p)!=5:\n",
    "        print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    preds = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        curr_sent = data[i]['value']['text']\n",
    "\n",
    "        prompt = f'''Your task is to perform classification of a given sentence derived from a court opinion. The classification serves the purpose of rhetorical roles assignment.\n",
    "\n",
    "Instructions:\n",
    "1. You need to classify the given sentence by associating it to exactly one rhetorical role.\n",
    "2. For reference, you will be provided with a sample set of semantically similar sentences along with their corresponding rhetorical roles.\n",
    "3. Choose rhetorical roles from provided sample or based on available information as preferred.\n",
    "\n",
    "Below we formalize rhetorical roles.\n",
    "RHETORICAL ROLES: # Try w/ and w/o lowercasing this or we can discard this too.\n",
    "Rhetorical roles in legal writing explain the varied purposes served by different document components, like a legal opinion, in conveying information, persuading readers, and crafting a coherent argument. These roles encompass various elements like factual background, legal principles, arguments, counter arguments, and conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "\n",
    "Following is the list of predefined rhetorical roles:\n",
    "['Preamble', 'Facts', 'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute', 'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "\n",
    "The definition of each of the stated rhetorical role is given below:\n",
    "Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "Facts: \tThis refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.). Depositions and proceedings of current court. Summary of lower court proceedings.\n",
    "Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES.\n",
    "Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS\n",
    "Statute : Text in which the court discusses Established laws, which can come from a mixture of sources - Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute\n",
    "Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent\n",
    "Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.\n",
    "Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum.\n",
    "Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale\n",
    "NONE: If a sentence does not belong to any of the above categories.\n",
    "\n",
    "'''    \n",
    "\n",
    "        trg_sent = \". \".join([item.strip() for item in curr_sent.strip().split(\"\\n\")])\n",
    "        prompt+=f\"Target sentence:\\n{trg_sent}\\n\\n\"\n",
    "\n",
    "        for p in top_preds[i]:\n",
    "            sent = \". \".join([item.strip() for item in p[0].strip().split(\"\\n\")])\n",
    "                    \n",
    "            prompt+= f\"sample_sentence: {sent}\" + f\"\\nassigned_role: {p[1][0] + p[1][1:].lower()}\\n\"\n",
    "\n",
    "        pred_prompt = f'''Do not generate anything except the rhetorical role for the given sentence.'''\n",
    "\n",
    "        prompt += pred_prompt\n",
    "    \n",
    "        response = get_completion(prompt).strip()\n",
    "        preds.append(response)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [03:35<00:00,  2.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels = run_preds(dataset[0]['annotations'][0]['result'])\n",
    "len(llm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Analysis',\n",
       "  'Argument by Respondent',\n",
       "  'Analysis',\n",
       "  'Analysis',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Ruling by Present Court',\n",
       "  'Preamble'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': \"'preamble'\",\n",
       " 'FAC': 'fac',\n",
       " 'RLC': 'rlc',\n",
       " 'ISSUE': 'issue',\n",
       " 'ARG_PETITIONER': 'arg_petitioner',\n",
       " 'ARG_RESPONDENT': 'arg_respondent',\n",
       " 'ANALYSIS': 'pre_analysis',\n",
       " 'STA': 'statute',\n",
       " 'PRE_RELIED': 'pre_relied',\n",
       " 'PRE_NOT_RELIED': 'pre_not_relied',\n",
       " 'Ratio': 'ratio_of_decision',\n",
       " 'RPC': 'rpc',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    \"'preamble'\": 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'fac': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'rlc': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'issue': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'argument_by_petitioner': 'ARG_PETITIONER',\n",
    "    'arg_petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'arg_respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'pre_analysis': 'ANALYSIS',\n",
    "    'statute': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'pre_relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'pre_not_relied': 'PRE_NOT_RELIED',\n",
    "    'ratio': 'Ratio',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ratio of decision': 'Ratio',\n",
    "    'ratio_of_decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'ruling_by_present_court': 'RPC',\n",
    "    'rpc': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "  \n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for i, label in enumerate(llm_labels):\n",
    "    try:\n",
    "        preds.append(label_abbrv[label.lower()])\n",
    "        trues.append(true_labels[i])\n",
    "    except:\n",
    "        # preds.append(\"none\")\n",
    "        # trues.append(true_labels[i].lower())\n",
    "        print(i, label.lower())\n",
    "        print(true_labels[i].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10], trues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "acc = accuracy_score(trues, preds)\n",
    "f1 = f1_score(trues, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37362637362637363, 0.34124113760336666)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.14      0.67      0.24         9\n",
      "ARG_PETITIONER       0.60      0.27      0.38        11\n",
      "ARG_RESPONDENT       0.50      0.06      0.11        17\n",
      "           FAC       0.43      0.83      0.57        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       0.00      0.00      0.00         2\n",
      "      PREAMBLE       0.67      1.00      0.80         4\n",
      "    PRE_RELIED       1.00      0.15      0.27        26\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       1.00      0.25      0.40         4\n",
      "           RPC       1.00      1.00      1.00         5\n",
      "\n",
      "      accuracy                           0.37        91\n",
      "     macro avg       0.49      0.39      0.34        91\n",
      "  weighted avg       0.65      0.37      0.35        91\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
