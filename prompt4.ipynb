{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGHCHAIN_TRACINB_V2 = \"true\"\n",
    "LANGCHAIN_API_KEY = \"ls__ef7c69da49cf48719e4d7c4b33a3debd\"\n",
    "import os\n",
    "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"train.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model='openchat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello! How can I help you today? If you have any questions or need assistance, please feel free to ask.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "top_preds = joblib.load('./vecs/top_preds')\n",
    "true_labels = joblib.load('./vecs/true_labels')\n",
    "doc_labels = joblib.load('./vecs/doc_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 91, 91)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_preds), len(true_labels), len(dataset[0]['annotations'][0]['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in top_preds:\n",
    "    if len(p)!=5:\n",
    "        print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      IN THE HIGH COURT OF KARNATAKA,\\n          CIRCUIT BENCH AT GULBARGA\\n\\nDATED THIS THE 22ND DAY OF FEBRUARY, 2013'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_sent = dataset[0]['annotations'][0]['result'][0]['value']['text']\n",
    "curr_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''You are a specialized system focused on sentence classification of court opinion.\n",
    "        RHETORICAL ROLE:\n",
    "        Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "        a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "        roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "        conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "        Your task is to label each sentence in the document with one of the following predefined rhetorical roles: ['Preamble', 'Facts',\n",
    "        'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute',\n",
    "        'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "        The definition of each rhetorical role is given below:\n",
    "        Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "        Facts: \tThis refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.). Depositions and proceedings of current court. Summary of lower court proceedings.\n",
    "        Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "        Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES.\n",
    "        Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "        Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "        Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS\n",
    "        Statute : Text in which the court discusses Established laws, which can come from a mixture of sources – Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute\n",
    "        Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent\n",
    "        Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.\n",
    "        Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum.\n",
    "        Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale\n",
    "        NONE: If a sentence does not belong to any of the above categories.\n",
    "        \n",
    "        You are given top 5 most similar sentences and their corresponding labels:\n",
    "\n",
    "        '''\n",
    "\n",
    "for p in top_preds[0]:\n",
    "    prompt+= f\"SENTENCE: {p[0].strip()}\" + f\"\\nLABEL: {p[1]}\\n\"\n",
    "\n",
    "pred_prompt = f'''Your task is to predict the label of the current sentence using the previous provided most similar sentences.\n",
    "Your output should only be the label of the current sentence as a string and nothing else.\n",
    "Now do prediction for this current sentence: {curr_sent.strip()}'''\n",
    "\n",
    "prompt += pred_prompt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a specialized system focused on sentence classification of court opinion.\n",
      "        RHETORICAL ROLE:\n",
      "        Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
      "        a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
      "        roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
      "        conclusions, each contributing to the document's overall persuasive and informative structure.\n",
      "        Your task is to label each sentence in the document with one of the following predefined rhetorical roles: ['Preamble', 'Facts',\n",
      "        'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute',\n",
      "        'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
      "        The definition of each rhetorical role is given below:\n",
      "        Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
      "        Facts: \tThis refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.). Depositions and proceedings of current court. Summary of lower court proceedings.\n",
      "        Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
      "        Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES.\n",
      "        Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
      "        Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
      "        Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS\n",
      "        Statute : Text in which the court discusses Established laws, which can come from a mixture of sources – Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute\n",
      "        Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent\n",
      "        Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.\n",
      "        Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum.\n",
      "        Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale\n",
      "        NONE: If a sentence does not belong to any of the above categories.\n",
      "        \n",
      "        You are given top 5 most similar sentences and their corresponding labels:\n",
      "\n",
      "        SENTENCE: IN THE HIGH COURT OF KARNATAKA\n",
      "                  DHARWAD BENCH\n",
      "\n",
      "DATED THIS THE 21ST DAY OF SEPTEMBER, 2015\n",
      "LABEL: PREAMBLE\n",
      "SENTENCE: IN THE HIGH COURT OF KARNATAKA AT BENGALURU\n",
      "\n",
      "     DATED THIS THE 19TH DAY OF JANUARY 2016\n",
      "LABEL: PREAMBLE\n",
      "SENTENCE: IN THE HIGH COURT OF KARNATAKA, BENGALURU\n",
      "\n",
      "      DATED THIS THE 20TH DAY OF JULY 2018\n",
      "LABEL: PREAMBLE\n",
      "SENTENCE: CIRCUIT BENCH AT DHARWAD\n",
      "\n",
      "   DATED THIS THE 29TH DAY OF JANUARY 2013\n",
      "LABEL: PREAMBLE\n",
      "SENTENCE: IN THE HIGH COURT OF KARNATAKA AT BENGALURU\n",
      "\n",
      "DATED THIS THE 26TH DAY OF FEBRUARY 2016\n",
      "LABEL: PREAMBLE\n",
      "Your task is to predict the label of the current sentence using the previous provided most similar sentences.\n",
      "Your output should only be the label of the current sentence as a string and nothing else.\n",
      "Now do prediction for this current sentence: IN THE HIGH COURT OF KARNATAKA,\n",
      "          CIRCUIT BENCH AT GULBARGA\n",
      "\n",
      "DATED THIS THE 22ND DAY OF FEBRUARY, 2013\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preamble'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preds(data):\n",
    "    preds = []\n",
    "    for i in tqdm(range(0, len(data))):\n",
    "        curr_sent = dataset[0]['annotations'][0]['result'][i]['value']['text']\n",
    "        \n",
    "\n",
    "        prompt = f'''You are a specialized system focused on sentence classification of court opinion.\n",
    "                RHETORICAL ROLE:\n",
    "                Rhetorical roles in legal writing refer to the distinct functions or purposes that different parts of a document, such as\n",
    "                a legal opinion, serve in conveying information, persuading the reader, and constructing a coherent argument. These\n",
    "                roles encompass various elements like factual background, legal principles, arguments, counter arguments, and\n",
    "                conclusions, each contributing to the document's overall persuasive and informative structure.\n",
    "                Your task is to label each sentence in the document with one of the following predefined rhetorical roles: ['Preamble', 'Facts',\n",
    "                'Ruling by Lower Court', 'Issues', 'Argument by Petitioner', 'Argument by Respondent', 'Analysis', 'Statute',\n",
    "                'Precedent Relied', 'Precedent Not Relied', 'Ratio of the decision', 'Ruling by Present Court', 'NONE']\n",
    "                The definition of each rhetorical role is given below:\n",
    "                Preamble: A typical judgement would start with the court name, the details of parties, lawyers and judges' names, Headnotes. This section typically would end with a keyword like (JUDGEMENT or ORDER etc.) Some supreme court cases also have HEADNOTES, ACTS section. They are also part of Preamble.\n",
    "                Facts: \tThis refers to the chronology of events (but not judgement by lower court) that led to filing the case, and how the case evolved over time in the legal system (e.g., First Information Report at a police station, filing an appeal to the Magistrate, etc.). Depositions and proceedings of current court. Summary of lower court proceedings.\n",
    "                Ruling by Lower Court: Judgments given by the lower courts (Trial Court, High Court) based on which the present appeal was made (to the Supreme Court or high court). The verdict of the lower Court, Analysis & the ratio behind the judgement by the lower Court is annotated with this label.\n",
    "                Issues: Some judgements mention the key points on which the verdict needs to be delivered. Such Legal Questions Framed by the Court are ISSUES.\n",
    "                Argument by Petitioner: Arguments by petitioners' lawyers. Precedent cases argued by petitioner lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "                Argument by Respondent: Arguments by respondents lawyers. Precedent cases argued by respondent lawyers fall under this but when court discusses them later then they belong to either the relied / not relied upon category.\n",
    "                Analysis: Courts discussion on the evidence,facts presented,prior cases and statutes. These are views of the court. Discussions on how the law is applicable or not applicable to current case. Observations(non binding) from court. It is the parent tag for 3 tags: PRE_RLEIED, PRE_NOT_RELIED and STATUTE i.e. Every statement which belong to these 3 tags should also be marked as ANALYSIS\n",
    "                Statute : Text in which the court discusses Established laws, which can come from a mixture of sources – Acts , Sections, Articles, Rules, Order, Notices, Notifications, Quotations directly from the bare act, and so on. Statute will have both the tags Analysis + Statute\n",
    "                Precedent Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were relied upon by the court for final decisions. So Precedent will have both the tags Analysis + Precedent\n",
    "                Precedent Not Relied: Sentences in which the court discusses prior case documents, discussions and decisions which were not relied upon by the court for final decisions. It could be due to the fact that the situation in that case is not relevant to the current case.\n",
    "                Ratio of the decision: Main Reason given for the application of any legal principle to the legal issue. This is the result of the analysis by the court. This typically appears right before the final decision. This is not the same as “Ratio Decidendi” taught in the Legal Academic curriculum.\n",
    "                Ruling by Present Court: Final decision + conclusion + order of the Court following from the natural / logical outcome of the rationale\n",
    "                NONE: If a sentence does not belong to any of the above categories.\n",
    "                \n",
    "                You are given top 5 most similar sentences and their corresponding labels:\n",
    "\n",
    "                '''\n",
    "\n",
    "        for p in top_preds[i]:\n",
    "            prompt+= f\"SENTENCE: {p[0].strip()}\" + f\"\\nLABEL: {p[1]}\\n\"\n",
    "\n",
    "        pred_prompt = f'''Your task is to predict the label of the current sentence.\n",
    "        You should use the most similar sentences and their labels to predict the label of the current sentence.\n",
    "        Your output should only be the label of the current sentence as a string and nothing else.\n",
    "        Now do prediction for this current sentence:\n",
    "        {curr_sent.strip()}'''\n",
    "\n",
    "        prompt += pred_prompt\n",
    "    \n",
    "        response = llm.invoke(prompt).strip()\n",
    "        preds.append(response)\n",
    "    \n",
    "    return preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:46<00:00,  1.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels = run_preds(dataset[0]['annotations'][0]['result'])\n",
    "len(llm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'Ruling by Present Court',\n",
       "  'RPC',\n",
       "  'Ruling by Present Court',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'],\n",
       " ['ANALYSIS',\n",
       "  'ANALYSIS',\n",
       "  'RATIO',\n",
       "  'ANALYSIS',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'RPC',\n",
       "  'NONE'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_labels[-10:], true_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PREAMBLE': 'preamble',\n",
       " 'FAC': 'fac',\n",
       " 'RLC': 'rlc',\n",
       " 'ISSUE': 'issue',\n",
       " 'ARG_PETITIONER': 'arg_petitioner',\n",
       " 'ARG_RESPONDENT': 'arg_respondent',\n",
       " 'ANALYSIS': 'analysis',\n",
       " 'STA': 'statute ',\n",
       " 'PRE_RELIED': 'pre_relied',\n",
       " 'PRE_NOT_RELIED': 'pre_not_relied',\n",
       " 'Ratio': 'ratio of the decision',\n",
       " 'RPC': 'rpc',\n",
       " 'NONE': 'none'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_abbrv = {\n",
    "    'preamble': 'PREAMBLE',\n",
    "    'facts': 'FAC',\n",
    "    'fac': 'FAC',\n",
    "    'ruling by lower court': 'RLC',\n",
    "    'rlc': 'RLC',\n",
    "    'issues': 'ISSUE',\n",
    "    'issue': 'ISSUE',\n",
    "    'argument by petitioner': 'ARG_PETITIONER',\n",
    "    'arg_petitioner': 'ARG_PETITIONER',\n",
    "    'argument by respondent': 'ARG_RESPONDENT',\n",
    "    'arg_respondent': 'ARG_RESPONDENT',\n",
    "    'analysis': 'ANALYSIS',\n",
    "    'statute ': 'STA',\n",
    "    'precedent relied': 'PRE_RELIED',\n",
    "    'pre_relied': 'PRE_RELIED',\n",
    "    'precedent not relied': 'PRE_NOT_RELIED',\n",
    "    'pre_not_relied': 'PRE_NOT_RELIED',\n",
    "    'ratio of the decision': 'Ratio',\n",
    "    'ruling by present court': 'RPC',\n",
    "    'rpc': 'RPC',\n",
    "    'none': 'NONE'\n",
    "}\n",
    "  \n",
    "abbrv_label = {v: k for k, v in label_abbrv.items()}\n",
    "abbrv_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 pre_ana\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "trues = []\n",
    "for i, label in enumerate(llm_labels):\n",
    "    try:\n",
    "        preds.append(label_abbrv[label.lower()])\n",
    "        trues.append(true_labels[i])\n",
    "    except:\n",
    "        print(i, label.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 90)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds), len(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'FAC',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'ANALYSIS',\n",
       "  'FAC'],\n",
       " ['PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'PREAMBLE',\n",
       "  'NONE',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC',\n",
       "  'FAC'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10], trues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(trues, preds)\n",
    "f1 = f1_score(trues, preds, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35555555555555557, 0.3746699207225523)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      ANALYSIS       0.17      0.67      0.27         9\n",
      "ARG_PETITIONER       0.33      0.27      0.30        11\n",
      "ARG_RESPONDENT       0.50      0.06      0.11        17\n",
      "           FAC       0.39      0.75      0.51        12\n",
      "         ISSUE       0.00      0.00      0.00         0\n",
      "          NONE       0.67      1.00      0.80         2\n",
      "      PREAMBLE       1.00      0.75      0.86         4\n",
      "PRE_NOT_RELIED       0.00      0.00      0.00         0\n",
      "    PRE_RELIED       1.00      0.04      0.08        25\n",
      "         RATIO       0.00      0.00      0.00         1\n",
      "           RLC       1.00      0.50      0.67         4\n",
      "           RPC       0.83      1.00      0.91         5\n",
      "\n",
      "      accuracy                           0.36        90\n",
      "     macro avg       0.49      0.42      0.37        90\n",
      "  weighted avg       0.63      0.36      0.31        90\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/silly_ronny/miniconda3/envs/langchain/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
